# -*- coding: utf-8 -*-
"""Project 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eAu-Xe6243QzmtkOsWzAxbtFv5y6TBfZ
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

spark = SparkSession.builder.appName('Project 3').getOrCreate()

"""### Schema"""

# Schema
viewing_schema = StructType([
    StructField("user_id", IntegerType(), True),
    StructField("apartment_id", IntegerType(), True),
    StructField("viewed_at", StringType(), True),
    StructField("is_wishlisted", BooleanType(), True),
    StructField("call_to_action", StringType(), True)
])

booking_schema = StructType([
    StructField("booking_id", IntegerType(), True),
    StructField("user_id", IntegerType(), True),
    StructField("apartment_id", IntegerType(), True),
    StructField("booking_date", StringType(), True),
    StructField("checkin_date", StringType(), True),
    StructField("checkout_date", StringType(), True),
    StructField("total_price", FloatType(), True),
    StructField("currency", StringType(), True),
    StructField("booking_status", StringType(), True)
])

property_schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("category", StringType(), True),
    StructField("body", StringType(), True),
    StructField("amenities", StringType(), True),
    StructField("bathrooms", IntegerType(), True),
    StructField("bedrooms", IntegerType(), True),
    StructField("fee", FloatType(), True),
    StructField("has_photo", BooleanType(), True),
    StructField("pets_allowed", BooleanType(), True),
    StructField("price_display", StringType(), True),
    StructField("price_type", StringType(), True),
    StructField("square_feet", IntegerType(), True),
    StructField("address", StringType(), True),
    StructField("cityname", StringType(), True),
    StructField("state", StringType(), True),
    StructField("latitude", FloatType(), True),
    StructField("longitude", FloatType(), True)
])

listing_schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("title", StringType(), True),
    StructField("source", StringType(), True),
    StructField("price", FloatType(), True),
    StructField("currency", StringType(), True),
    StructField("listing_created_on", StringType(), True),
    StructField("is_active", BooleanType(), True),
    StructField("last_modified_timestamp", StringType(), True)
])

"""## Load Data"""

# Load data (assuming data is in CSV format)
viewings_df = spark.read.option("header", True).schema(viewing_schema).csv("/content/user_viewing.csv")
bookings_df = spark.read.option("header", True).schema(booking_schema).csv("/content/bookings.csv")
properties_df = spark.read.option("header", True).schema(property_schema).csv("/content/apartment_attributes.csv")
listings_df = spark.read.option("header", True).schema(listing_schema).csv("/content/apartments.csv")

# Print schemas
viewings_df.printSchema()
bookings_df.printSchema()
properties_df.printSchema()
listings_df.printSchema()

# Drop unecessary columns
viewings_df = viewings_df.drop("is_wishlisted", "call_to_action")
bookings_df = bookings_df.drop("booking_id", "currency")
properties_df = properties_df.drop("category", "body", "amenities", "bathrooms", "bedrooms",
                                  "fee", "has_photo", "pets_allowed", "price_display",
                                  "price_type", "square_feet", "address", "state",
                                  "latitude", "longitude")
listings_df = listings_df.drop("title", "source", "last_modified_timestamp")

# Convert date strings to date type
viewings_df = viewings_df.withColumn("viewed_at", to_date("viewed_at", "dd/MM/yyyy"))
bookings_df = bookings_df.withColumn("booking_date", to_date("booking_date", "dd/MM/yyyy")) \
    .withColumn("checkin_date", to_date("checkin_date", "dd/MM/yyyy")) \
    .withColumn("checkout_date", to_date("checkout_date", "dd/MM/yyyy"))
listings_df = listings_df.withColumn("listing_created_on", to_date("listing_created_on", "dd/MM/yyyy"))

# Calculate booking duration with absolute value
bookings_df = bookings_df.withColumn("booking_duration",
    abs(datediff("checkout_date", "checkin_date")))

# Rental Performance Metrics

# 1. Average Listing Price (weekly)
avg_price_weekly = listings_df.filter(col("is_active") == True) \
    .withColumn("week", weekofyear("listing_created_on")) \
    .groupBy("week") \
    .agg(avg("price").alias("avg_listing_price")) \
    .orderBy("week")

avg_price_weekly.show()

# 2. Occupancy Rate (monthly)
confirmed_bookings = bookings_df.filter(col("booking_status") == "confirmed")
monthly_occupancy = confirmed_bookings \
    .withColumn("month", month("checkin_date")) \
    .groupBy("month") \
    .agg(
        (sum("booking_duration") / 30).alias("occupied_days_ratio")
    ) \
    .withColumn("occupancy_rate", col("occupied_days_ratio") * 100) \
    .orderBy("month")

monthly_occupancy.show()

# 3. Most Popular Locations (weekly)
popular_locations = confirmed_bookings \
    .join(properties_df, confirmed_bookings.apartment_id == properties_df.id) \
    .withColumn("week", weekofyear("checkin_date")) \
    .groupBy("week", "cityname") \
    .agg(count("*").alias("booking_count")) \
    .orderBy("week", desc("booking_count"))

popular_locations.show()

# 4. Top Performing Listings (weekly)
top_listings = confirmed_bookings \
    .withColumn("week", weekofyear("checkin_date")) \
    .groupBy("week", "apartment_id") \
    .agg(sum("total_price").alias("revenue")) \
    .orderBy("week", desc("revenue"))

top_listings.show()

# User Engagement Metrics

# 1. Total Bookings per User (weekly)
bookings_per_user = bookings_df \
    .withColumn("week", weekofyear("booking_date")) \
    .groupBy("week", "user_id") \
    .agg(count("*").alias("booking_count")) \
    .orderBy("week", "user_id")
bookings_per_user.show()

# 2. Average Booking Duration
avg_booking_duration = confirmed_bookings \
    .groupBy(month("checkin_date").alias("month")) \
    .agg(avg("booking_duration").alias("avg_duration_days")) \
    .orderBy("month")
avg_booking_duration.show()

from pyspark.sql.window import Window

# Convert booking_date to Unix timestamp for numeric range compatibility
bookings_df_with_ts = bookings_df.withColumn("booking_ts", unix_timestamp("booking_date"))

# Define the window using the timestamp (30 days = 30 * 24 * 60 * 60 seconds = 2,592,000 seconds)
windowSpec = Window.partitionBy("user_id").orderBy("booking_ts").rangeBetween(-2592000, 0)

# Calculate repeat customer rate
repeat_customers = bookings_df_with_ts \
    .withColumn("booking_count_30d", count("*").over(windowSpec)) \
    .withColumn("is_repeat", when(col("booking_count_30d") > 1, 1).otherwise(0)) \
    .groupBy(month("booking_date").alias("month")) \
    .agg(
        (sum("is_repeat") / countDistinct("user_id") * 100).alias("repeat_rate_percent")
    ) \
    .orderBy("month")

repeat_customers.show()

spark.stop()

